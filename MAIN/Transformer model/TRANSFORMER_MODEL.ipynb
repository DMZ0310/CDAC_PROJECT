{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84e73aa3",
   "metadata": {},
   "source": [
    "# create metadata CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f1b268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV created with 4281 samples\n",
      "                  file label\n",
      "0  1001_DFA_ANG_XX.wav   ANG\n",
      "1  1001_DFA_DIS_XX.wav   DIS\n",
      "2  1001_DFA_FEA_XX.wav   FEA\n",
      "3  1001_DFA_HAP_XX.wav   HAP\n",
      "4  1001_DFA_NEU_XX.wav   NEU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "AUDIO_DIR = r\"C:\\Users\\Dennismz\\Desktop\\CDAC_PROJECT\\dataset for SER\\CREMA-D\\AudioWAV\"\n",
    "\n",
    "rows = []\n",
    "for file in os.listdir(AUDIO_DIR):\n",
    "    if file.endswith(\".wav\"):\n",
    "        emotion = file.split(\"_\")[2]  # ANG, HAP, SAD, etc.\n",
    "        rows.append({\n",
    "            \"file\": file,\n",
    "            \"label\": emotion\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"transcripts.csv\", index=False)\n",
    "\n",
    "print(\"CSV created with\", len(df), \"samples\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc41c4d6",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95b769e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dennismz\\anaconda3\\envs\\dataenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d034f42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Dataset\n",
    "# =========================\n",
    "class SERDataset(Dataset):\n",
    "    def __init__(self, csv_path, audio_dir, sr=16000, max_seconds=6):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.sr = sr\n",
    "        self.max_len = sr * max_seconds\n",
    "\n",
    "        self.le = LabelEncoder()\n",
    "        self.labels = self.le.fit_transform(self.df[\"label\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.df.iloc[idx][\"file\"]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        path = os.path.join(self.audio_dir, file)\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(path)\n",
    "\n",
    "        y, _ = librosa.load(path, sr=self.sr, mono=True)\n",
    "\n",
    "        if len(y) > self.max_len:\n",
    "            y = y[:self.max_len]\n",
    "        else:\n",
    "            y = np.pad(y, (0, self.max_len - len(y)))\n",
    "\n",
    "        return {\n",
    "            \"audio\": torch.tensor(y, dtype=torch.float32),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55af45c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Feature Extraction\n",
    "# =========================\n",
    "def collate_fn(batch, extractor, wav2vec):\n",
    "    audios = [b[\"audio\"].numpy() for b in batch]\n",
    "    labels = torch.stack([b[\"label\"] for b in batch]).to(DEVICE)\n",
    "\n",
    "    inputs = extractor(\n",
    "        audios,\n",
    "        sampling_rate=16000,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = wav2vec(**inputs)\n",
    "        features = outputs.last_hidden_state  # (B, T, 768)\n",
    "\n",
    "    return features, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47c80d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Model\n",
    "# =========================\n",
    "class CNN_BiLSTM(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(768, 128, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=256,\n",
    "            hidden_size=256,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)          # (B, 768, T)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.permute(0, 2, 1)          # (B, T, 256)\n",
    "\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.mean(dim=1)               # Temporal pooling\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60e8dcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dennismz\\anaconda3\\envs\\dataenv\\lib\\site-packages\\transformers\\configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|██████████| 856/856 [1:01:51<00:00,  4.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 0.3373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 856/856 [1:01:02<00:00,  4.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 0.4095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 856/856 [56:40<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 0.4384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 856/856 [54:50<00:00,  3.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 0.4743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 856/856 [57:01<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 856/856 [51:26<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 0.5091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 856/856 [46:47<00:00,  3.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 0.5315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 856/856 [52:15<00:00,  3.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 0.5459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 856/856 [55:22<00:00,  3.88s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 0.5567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 856/856 [50:45<00:00,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 0.5698\n",
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Training\n",
    "# =========================\n",
    "def train():\n",
    "    CSV = \"transcripts.csv\"\n",
    "    AUDIO_DIR = r\"C:\\Users\\Dennismz\\Desktop\\CDAC_PROJECT\\dataset for SER\\CREMA-D\\AudioWAV\"\n",
    "    EPOCHS = 10\n",
    "    BATCH = 4\n",
    "\n",
    "    dataset = SERDataset(CSV, AUDIO_DIR)\n",
    "\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        range(len(dataset)),\n",
    "        test_size=0.2,\n",
    "        stratify=dataset.labels,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    train_ds = Subset(dataset, train_idx)\n",
    "    val_ds = Subset(dataset, val_idx)\n",
    "\n",
    "    extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
    "        \"facebook/wav2vec2-base\"\n",
    "    )\n",
    "\n",
    "    wav2vec = Wav2Vec2Model.from_pretrained(\n",
    "        \"facebook/wav2vec2-base\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Freeze wav2vec\n",
    "    for p in wav2vec.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    model = CNN_BiLSTM(num_classes=len(dataset.le.classes_)).to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=BATCH,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda x: collate_fn(x, extractor, wav2vec)\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=BATCH,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda x: collate_fn(x, extractor, wav2vec)\n",
    "    )\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        total, correct, loss_sum = 0, 0, 0\n",
    "\n",
    "        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "            correct += (out.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "        print(f\"Train Acc: {correct/total:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"ser_model.pt\")\n",
    "    print(\"Model saved.\")\n",
    "\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dataenv)",
   "language": "python",
   "name": "dataenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
